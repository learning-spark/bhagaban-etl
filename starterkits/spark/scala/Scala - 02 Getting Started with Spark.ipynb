{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scala - Getting Started with Spark\n",
    "\n",
    "As part of this module we will take a simple use case and try to scratch the surface of the Spark.\n",
    "\n",
    "* Understand Data Model\n",
    "* Define Problem Statement\n",
    "* Creating Spark Context\n",
    "* Setting Run Time Job Properties\n",
    "* Reading data from CSV Files\n",
    "* Apply Filtering\n",
    "* Row Level Transformations\n",
    "* Perform Joins\n",
    "* Aggregate Data\n",
    "* Perform Sorting\n",
    "* Write output to Files\n",
    "* Complete Script\n",
    "* Validating Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand Data Model\n",
    "\n",
    "Let us understand the data model and also characteristics of the data.\n",
    "\n",
    "* Base directory for **retail_db** data sets is **/public/retail_db**.\n",
    "* It have six folders, each folder represents a separate table.\n",
    "  * Product Catalog Tables\n",
    "    * products\n",
    "    * categories\n",
    "    * departments\n",
    "  * Customers Table\n",
    "    * customers\n",
    "  * Transactional Tables\n",
    "    * orders\n",
    "    * order_items\n",
    "* **orders** and **order_items** are related. **orders** is parent table and **order_items** is child table for orders.\n",
    "* All folders have one ore more files under them.\n",
    "* Each line represents a record and have values related to multiple columns. Each record is delimited or separated by **comma (,)**.\n",
    "* First field in each orders record is order_id and it is a primary key (unique and not null)\n",
    "* Second field in each order_items record is order_item_order_id which is a foreign key attribute to orders order_id.\n",
    "* There are other relationships as well, however they are not relevant to get started. We will primarily focus on orders and order_items data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Problem Statement\n",
    "\n",
    "Get monthly revenue considering complete or closed orders\n",
    "\n",
    "* We will use orders and order_items data.\n",
    "* **orders** is available at **/public/retail_db/orders**\n",
    "* **order_items** is available at **/public/retail_db/order_items**\n",
    "* We need to consider orders with COMPLETE or CLOSED status.\n",
    "* Revenue can be computed using **order_item_subtotal** which is 5th attribute in order_items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Spark Context\n",
    "\n",
    "Let us understand how to create Spark Context using `SparkSession` from `org.apache.spark.sql`.\n",
    "\n",
    "* We need to have spark context to leverage both APIs as well as distributed computing framework.\n",
    "* `SparkSession` is a wrapper class which will use existing Spark Context or create new one.\n",
    "* We can customize the behavior of Spark Context created by passing properties using `config` or by using APIs such as `appName`, `master` etc.\n",
    "* APIs are provided only for most commonly used properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    appName(\"Getting Started - Monthly Revenue\").\n",
    "    master(\"yarn\").\n",
    "    getOrCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Run Time Job Properties\n",
    "\n",
    "Let us understand how to customize run time behavior of submitted jobs.\n",
    "\n",
    "* Once Spark Context is created, we can customize run time behavior by using `spark.conf.set`. \n",
    "* In our case let us set a property called as `spark.sql.shuffle.partitions` to 2.\n",
    "* If we do not set this property, by default it will use 200 threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from CSV Files\n",
    "\n",
    "Let us quickly see how we can read data from CSV Files.\n",
    "\n",
    "* Spark provide several APIs to read the files of different file formats.\n",
    "* All the out of the box APIs are available under `spark.read`.\n",
    "* In our case we have to read text files where each record is delimited or separated by comma (',').\n",
    "* To create Data Frames for `orders` and `order_items` we can pass the path to `spark.read.csv`.\n",
    "* There are other options as well which can be passed using keyword arguments. You can run help on `spark.read.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Reading orders\n",
    "val orders_path = \"/public/retail_db/orders\"\n",
    "val orders = spark.\n",
    "    read.\n",
    "    schema(\"order_id INT, order_date STRING, \" +\n",
    "           \"order_customer_id INT, order_status STRING\"\n",
    "          ).\n",
    "    csv(orders_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Reading order_items\n",
    "val order_items_path = \"/public/retail_db/order_items\"\n",
    "val order_items = spark.\n",
    "    read.\n",
    "    schema(\"order_item_id INT, order_item_order_id INT, \" +\n",
    "           \"order_item_product_id INT, order_item_quantity INT, \" +\n",
    "           \"order_item_subtotal FLOAT, order_item_product_price FLOAT\"\n",
    "          ).\n",
    "    csv(order_items_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Filtering\n",
    "\n",
    "Let us see how we can filter out records in Data Frame.\n",
    "* We can either use `filter` or `where` to filter the data. Both of them serve the same purpose.\n",
    "* We can pass the condictions either in SQL Style or API Style.\n",
    "* In this case, we have used SQL Style to check `order_status` for `COMPLETE` or `CLOSED` orders.\n",
    "* We can perform all standard filtering conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orders_filtered = orders.\n",
    "    filter(\"order_status in ('COMPLETE', 'CLOSED')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_filtered.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here is an example for API Style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.\n",
    "    filter(orders(\"order_status\").isin(\"COMPLETE\", \"CLOSED\")).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row Level Transformations\n",
    "\n",
    "Let us see how we can project and also derive new fields out of existing fields leveraging functions.\n",
    "\n",
    "* One of the ways to project data is by using `select` on top of Data Frame. We have to pass the column names as column type. Spark Implicits provide `$` operator to pass column names as column type.\n",
    "* We can also pass column names as column type using `col` function as well as by passing column names like this `orders(\"order_id\")`.\n",
    "* Spark provides almost 300 pre defined functions as part of `org.apache.spark..sql.functions`.\n",
    "* In our case we need to import and use `date_format` function to extract month from existing date.\n",
    "* Later we will also import and use functions such as `sum` and `round` while aggregating the data.\n",
    "* We can also provide meaningful names to derived fields using `alias`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.date_format\n",
    "\n",
    "val orders_transformed = orders_filtered.\n",
    "    select($\"order_id\", date_format($\"order_date\", \"yyyyMM\").alias(\"order_month\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_transformed.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Joins\n",
    "\n",
    "Let us join both the data sets which have the fields we are interested in. \n",
    "\n",
    "* We can join data sets using `join`.\n",
    "* We also might have to pass join condition in case the column names are different between the data sets.\n",
    "* In our case we have to join `orders` and `order_items` using `orders(\"order_id\")` and `order_items(\"order_item_order_id\")`.\n",
    "\n",
    "We can join original Data Frames as well and generate order_month while grouping the data as demonstrated in the **Complete Script** Section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val order_details_by_month = orders_transformed.\n",
    "    join(order_items, \n",
    "         orders(\"order_id\") === order_items(\"order_item_order_id\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_details_by_month.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Data\n",
    "\n",
    "As we have joined `orders` and `order_items`, let us perform the aggregation.\n",
    "* In this case want to compute revenue for each month.\n",
    "* `order_month` is derived field which contain both year and month.\n",
    "* We can use `order_month` as part of `groupBy` so that data can be grouped. It will generate a special Data Frame of type `GroupedData`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_details_by_month.\n",
    "    groupBy(\"order_month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can now invoke aggregate functions such as `sum` and pass the desired field using which we want to aggregate (in this case we can pass `order_item_subtotal` to `sum`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.{sum, round}\n",
    "\n",
    "val monthly_revenue = order_details_by_month.\n",
    "    groupBy(\"order_month\").\n",
    "    agg(round(sum(\"order_item_subtotal\"), 2).alias(\"revenue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_revenue.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Sorting\n",
    "\n",
    "As we got the revenue for each month, let us sort the data so that we can review the output for the validation.\n",
    "\n",
    "* We can use `orderBy` or `sort` to sort the data.\n",
    "* By default data will be sorted in ascending order.\n",
    "* In this case we are sorting the data by `order_month`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val monthly_revenue_sorted = monthly_revenue.\n",
    "    orderBy(\"order_month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_revenue_sorted.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write output to Files\n",
    "\n",
    "As data is read, processed and sorted - now it is time to write data to files in underlying file system.\n",
    "\n",
    "* In our environment **/public** is read only folder. You will not be able to add files under subdirectories of **/public**.\n",
    "* Assuming you have write access to **/user/[OS_USER_NAME]**, I have used **/user/{username}/retail_db/monthly_revenue** as target folder.\n",
    "* `${username}` is replaced by the OS user used for login using `System.getProperty(\"user.name\")`.\n",
    "* `coalesce(1)` is used to write the output to one file.\n",
    "* If the folder and files already exists, `mode(\"overwrite\")` will replace existing folder with new files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val username = System.getProperty(\"user.name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_revenue_sorted.\n",
    "    coalesce(1).\n",
    "    write.\n",
    "    mode(\"overwrite\").\n",
    "    option(\"header\", \"true\").\n",
    "    csv(s\"/user/${username}/retail_db/monthly_revenue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Script\n",
    "\n",
    "Here is the complete script or program which takes care of the following:\n",
    "\n",
    "* Create Spark Context and set the properties.\n",
    "* Read the data related to different tables.\n",
    "* Process the data using relevant Spark Data Frame APIs.\n",
    "* Write the data back to file system\n",
    "\n",
    "Entire Data processing and writing the data back to file system is developed using Piped approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "username = training\n",
       "spark = org.apache.spark.sql.SparkSession@410aecbf\n",
       "orders_path = /public/retail_db/orders\n",
       "orders = [order_id: int, order_date: string ... 2 more fields]\n",
       "order_items_path = /public/retail_db/order_items\n",
       "order_items = [order_item_id: int, order_item_order_id: int ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_item_id: int, order_item_order_id: int ... 4 more fields]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions.{date_format, sum, round}\n",
    "\n",
    "val username = System.getProperty(\"user.name\")\n",
    "\n",
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    appName(\"Getting Started - Monthly Revenue\").\n",
    "    master(\"yarn\").\n",
    "    getOrCreate\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "import spark.implicits._\n",
    "\n",
    "// Reading orders\n",
    "val orders_path = \"/public/retail_db/orders\"\n",
    "val orders = spark.\n",
    "    read.\n",
    "    schema(\"order_id INT, order_date STRING, \" +\n",
    "           \"order_customer_id INT, order_status STRING\"\n",
    "          ).\n",
    "    csv(orders_path)\n",
    "\n",
    "// Reading order_items\n",
    "val order_items_path = \"/public/retail_db/order_items\"\n",
    "val order_items = spark.\n",
    "    read.\n",
    "    schema(\"order_item_id INT, order_item_order_id INT, \" +\n",
    "           \"order_item_product_id INT, order_item_quantity INT, \" +\n",
    "           \"order_item_subtotal FLOAT, order_item_product_price FLOAT\"\n",
    "          ).\n",
    "    csv(order_items_path)\n",
    "\n",
    "orders.\n",
    "    filter(\"order_status in ('COMPLETE', 'CLOSED')\").\n",
    "    join(order_items, orders(\"order_id\") === order_items(\"order_item_order_id\")).\n",
    "    groupBy(date_format($\"order_date\", \"yyyyMM\").alias(\"order_month\")).\n",
    "    agg(round(sum($\"order_item_subtotal\"), 2).alias(\"revenue\")).\n",
    "    orderBy(\"order_month\").\n",
    "    coalesce(1).\n",
    "    write.\n",
    "    mode(\"overwrite\").\n",
    "    option(\"header\", \"true\").\n",
    "    csv(s\"/user/${username}/retail_db/monthly_revenue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating Output\n",
    "\n",
    "Let us go ahead and validate the output.\n",
    "\n",
    "* In our case we are using HDFS and hence we should be able to use HDFS commands to validate.\n",
    "* Let us first list the files which will give some idea about when they are created.\n",
    "* For some file formats, we will also see extension as well as compression algorithm used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   2 training training        252 2020-03-20 14:50 /user/training/retail_db/monthly_revenue/part-00000-62d47b0c-ecbc-46f6-baee-bc35761706a8-c000.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "username = training\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys.process._\n",
    "\n",
    "val username = System.getProperty(\"user.name\")\n",
    "\n",
    "s\"hdfs dfs -ls /user/${username}/retail_db/monthly_revenue/part*\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In case of small text files we can use `cat` to see the contents. It might not work if the files are compressed.\n",
    "* Also, it is not a good practice to use `cat` for larger text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_month,revenue\n",
      "201307,333465.46\n",
      "201308,1221828.92\n",
      "201309,1302255.83\n",
      "201310,1171686.94\n",
      "201311,1379935.36\n",
      "201312,1277719.63\n",
      "201401,1230221.76\n",
      "201402,1217770.11\n",
      "201403,1271350.99\n",
      "201404,1249723.54\n",
      "201405,1221679.35\n",
      "201406,1179754.08\n",
      "201407,955590.79\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "username = training\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys.process._\n",
    "\n",
    "val username = System.getProperty(\"user.name\")\n",
    "\n",
    "s\"hdfs dfs -cat /user/${username}/retail_db/monthly_revenue/part*\"!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
