{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping using BeautifulSoup\n",
    "\n",
    "As part of this module we will see how to scrape the data from web pages.\n",
    "* Problem Statement\n",
    "* Installing Pre-requisites\n",
    "* Overview of BeautifulSoup\n",
    "* Getting HTML Content\n",
    "* Processing HTML Content\n",
    "* Creating Data Frame\n",
    "* Processing Data using Data Frame APIs\n",
    "* Exercise - Airport Traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Let us define a problem statement for Web Scraping. We will primarily focus on BeautifulSoup.\n",
    "\n",
    "* Vande Bharat Flight Service is Indian Government sponsored service. They open up flights in regular intervals amidst of Covid19 Pandemic. Here is the [link](https://mea.gov.in/phase-6.htm) for one of the published schedule.\n",
    "* However their website is static and we are not able to easily figure out the details related to flights from the source to destination.\n",
    "* We want to scrape this static page and try to extract the information we are looking for. Ideally we can build a website with some additional filter criteria.\n",
    "* However as we want to just explore Web Scraping using Beautiful Soup, we will go to the extent of reading the HTML table into Pandas Data Frame and run some basic queries.\n",
    "  * Get all Columns\n",
    "  * Get all unique destinations. They are nothing but Indian airports.\n",
    "  * Get all the distinct origins with **Country of Origin** as **USA**.\n",
    "  * Get all the distinct airports in US from which flights are operated. You have to get all the origins which are not reflected in destinations.\n",
    "  * Get all the flights that are available beyond passed date from US."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Pre-requisites\n",
    "\n",
    "We will use multiple Python libraries to perform Web Scraping.\n",
    "* Library to get the content from HTML Pages **requests**\n",
    "* Process HTML Tags and extract Data **beautifulsoup4**\n",
    "* Data Processing using Data Frame APIs **pandas**\n",
    "\n",
    "```\n",
    "pip install beautifulsoup4\n",
    "pip install pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of BeautifulSoup\n",
    "\n",
    "Let us get brief overview of BeautifulSoup.\n",
    "* We will create a simple HTML Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <th>Details</th>\n",
       "            <th>URL</th>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Video Content</td>\n",
       "            <td><a href=\"https://www.youtube.com/itversityin\">YouTube Channel</a></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Reference Material</td>\n",
       "            <td><a href=\"https://www.github.com/dgadiraju/itversity-books\">GitHub Repository</a></td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th>Details</th>\n",
    "            <th>URL</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Video Content</td>\n",
    "            <td><a href=\"https://www.youtube.com/itversityin\">YouTube Channel</a></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Reference Material</td>\n",
    "            <td><a href=\"https://www.github.com/dgadiraju/itversity-books\">GitHub Repository</a></td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_str = \"\"\"<table>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <th>Details</th>\n",
    "            <th>URL</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Video Content</td>\n",
    "            <td><a href=\"https://www.youtube.com/itversityin\">YouTube Channel</a>\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Reference Material</td>\n",
    "            <td><a href=\"https://www.github.com/dgadiraju/itversity-books\">GitHub Repository</a>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create BeautifulSoup object by name soup.\n",
    "* We can access first occurrence of tag using its reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table>\n",
      " <tbody>\n",
      "  <tr>\n",
      "   <th>\n",
      "    Details\n",
      "   </th>\n",
      "   <th>\n",
      "    URL\n",
      "   </th>\n",
      "  </tr>\n",
      "  <tr>\n",
      "   <td>\n",
      "    Video Content\n",
      "   </td>\n",
      "   <td>\n",
      "    <a href=\"https://www.youtube.com/itversityin\">\n",
      "     YouTube Channel\n",
      "    </a>\n",
      "   </td>\n",
      "  </tr>\n",
      "  <tr>\n",
      "   <td>\n",
      "    Reference Material\n",
      "   </td>\n",
      "   <td>\n",
      "    <a href=\"https://www.github.com/dgadiraju/itversity-books\">\n",
      "     GitHub Repository\n",
      "    </a>\n",
      "   </td>\n",
      "  </tr>\n",
      " </tbody>\n",
      "</table>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html_str, 'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accessing first occurrence of `tr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tr>\n",
       "<th>Details</th>\n",
       "<th>URL</th>\n",
       "</tr>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.table.tbody.tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accessing first `th` value, we can use `string` or `get_text()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Details'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.table.tbody.tr.th.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Details'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.table.tbody.tr.th.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accessing first occurrence of anchor tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"https://www.youtube.com/itversityin\">YouTube Channel</a>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.table.tbody.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Getting the url from `href` attribute of anchor tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.youtube.com/itversityin'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.table.tbody.a['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accessing the value of anchor tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'YouTube Channel'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.table.tbody.a.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get all anchor tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"https://www.youtube.com/itversityin\">YouTube Channel</a>,\n",
       " <a href=\"https://www.github.com/dgadiraju/itversity-books\">GitHub Repository</a>]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.table.tbody.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get all `td` tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<td>Video Content</td>\n",
      "<td><a href=\"https://www.youtube.com/itversityin\">YouTube Channel</a>\n",
      "</td>\n",
      "<td>Reference Material</td>\n",
      "<td><a href=\"https://www.github.com/dgadiraju/itversity-books\">GitHub Repository</a>\n",
      "</td>\n"
     ]
    }
   ],
   "source": [
    "for a in soup.find_all('td'):\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get value from all `td` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Content\n",
      "None\n",
      "Reference Material\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# If the text in the tag have characters like new line, string might return None\n",
    "for td in soup.find_all('td'):\n",
    "    print(td.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Content\n",
      "YouTube Channel\n",
      "\n",
      "Reference Material\n",
      "GitHub Repository\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If the text in the tag have characters like new line, we can use get_text\n",
    "for td in soup.find_all('td'):\n",
    "    print(td.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Content\n",
      "YouTube Channel\n",
      "Reference Material\n",
      "GitHub Repository\n"
     ]
    }
   ],
   "source": [
    "# Stripping new line characters\n",
    "for td in soup.find_all('td'):\n",
    "    print(td.get_text().rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get values and URLs from anchor tags as a list of dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'description': 'YouTube Channel',\n",
       "  'url': 'https://www.youtube.com/itversityin'},\n",
       " {'description': 'GitHub Repository',\n",
       "  'url': 'https://www.github.com/dgadiraju/itversity-books'}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itversity_details = []\n",
    "for a in soup.find_all('a'):\n",
    "    rec = {'description': a.string, 'url': a['href']}\n",
    "    itversity_details.append(rec)\n",
    "\n",
    "itversity_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting HTML Content\n",
    "\n",
    "We can use Python core library `requests` to get the content from HTML pages.\n",
    "* `requests` provides `get` funcion to which we can pass web URL. - `flights_page = requests.get(flights_url)`\n",
    "* We can access content using `flights_page.content`.\n",
    "* We can use pass the content to BeautifulSoup and parse the HTML Tags and data for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_url = 'https://mea.gov.in/phase-6.htm'\n",
    "flights_page = requests.get(flights_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing HTML Content\n",
    "\n",
    "We can process the content and extract HTML Tags as well as data using BeautifulSoup.\n",
    "* We have to pass the content using `html.parser` and build the BeautifulSoup object.\n",
    "* Let us prettify and print the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(flights_page.content, 'html.parser')\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let us extract all the `th` tags. It will give us header of the table.\n",
    "* We have multiple `tr` tags with `th` and we need to consider only that `tr` tag with 12 elements.\n",
    "* Here is the code snippet to get the header of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tr in soup.find_all('tr'):\n",
    "    th = tr.find_all('th')\n",
    "    if len(th) == 12:\n",
    "        for field_name in th:\n",
    "            print(field_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can use `field_name.string` to get only the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tr in soup.find_all('tr'):\n",
    "    th = tr.find_all('th')\n",
    "    if len(th) == 12:\n",
    "        for field_name in th:\n",
    "            print(field_name.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also get the actual values from `td` tags.\n",
    "* We will only get 3 of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr = 0\n",
    "for tr in soup.find_all('tr'):\n",
    "    if ctr == 3: break\n",
    "    td = tr.find_all('td')\n",
    "    if len(td) == 12:\n",
    "        for field_name in td:\n",
    "            print(field_name.string)\n",
    "        ctr += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data Frame\n",
    "\n",
    "Let us build the Data Frame so that we can process the data using Data Frame APIs.\n",
    "* We will get all the headers into a list **field_names** by using data from `th` tags.\n",
    "* We will get all the `tr` tags with `td` tags. We will build list **field_values** using one row at a time.\n",
    "* While processing **table rows** we will build the dict using **field_names** and **field_values**. Using these dicts, we will build a list of dicts\n",
    "* Using list of dicts we will create the Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list for field names.\n",
    "field_names = []\n",
    "\n",
    "for tr in soup.find_all('tr'):\n",
    "    th = tr.find_all('th')\n",
    "    if len(th) == 12:\n",
    "        for field_name in th:\n",
    "            field_names.append(field_name.string)\n",
    "\n",
    "field_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have list of tuples with 2 elements we can create dict as below \n",
    "l = [(1, 'Hello'), (2, 'World')]\n",
    "dict(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have 2 lists, we can merge into one list of paired tuples using zip as below.\n",
    "# We will use this approach to build dic\n",
    "l1 = [1, 2]\n",
    "l2 = ['Hello', 'World']\n",
    "\n",
    "dict(zip(l1, l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list of dicts. Each dict will contain 12 elements with keys from field_names and values from field_values\n",
    "data = []\n",
    "for tr in soup.find_all('tr'):\n",
    "    td = tr.find_all('td')\n",
    "    field_values = []\n",
    "    if len(td) == 12:\n",
    "        for field_value in td:\n",
    "            field_values.append(field_value.string)\n",
    "        rec = dict(zip(field_names, field_values))\n",
    "        data.append(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Pandas Data Frame\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data using Data Frame APIs\n",
    "Here are the problem statements for which we will try to come up with the solution\n",
    "* Get all Columns\n",
    "* Get all unique destinations. They are nothing but Indian airports.\n",
    "* Get all the distinct origins with **Country of Origin** as **USA**.\n",
    "* Get all the distinct airports in US from which flights are operated. You have to get all the origins which are not reflected in destinations.\n",
    "* Get all the flights that are available beyond passed date from US."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all Columns\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique destinations\n",
    "unique_destinations = df['Destination'].unique()\n",
    "\n",
    "unique_destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the distinct airports in US from which flights are operated. You have to get all the origins which are not reflected in destinations.\n",
    "unique_origins = df.query('`Country of Origin` == \"USA\"')['Origin'].unique()\n",
    "\n",
    "unique_origins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(unique_origins).difference(set(unique_destinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the flights that are available beyond passed date from US.\n",
    "df.query('`Country of Origin` == \"USA\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "datetime.datetime.strptime('2-Sep-20', '%d-%b-%y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Dep Date'] = pd.to_datetime(df['Dep Date'], format='%d-%b-%y')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.query('`Country of Origin` == \"USA\" & `Dep Date`.dt.strftime(\"%Y-%m-%d\") > \"2020-09-03\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Airport Traffic\n",
    "\n",
    "Use [HTML File](https://raw.githubusercontent.com/dgadiraju/itversity-books/master/Data%20Engineering%20Bootcamp/30%20Basics%20of%20Programming%20using%20Python/11%20Exercise%20-%20Web%20Scraping%20-%20Airports%20Data.html) and get the data into the Data Frame with these fields. We need to unpivot and get the air traffic by year.\n",
    "\n",
    "* IATA Code\n",
    "* Major city served\n",
    "* State\n",
    "* Year\n",
    "* Air Traffic\n",
    "\n",
    "**Hint: You can use Pandas melt function to unpivot the data**\n",
    "\n",
    "Output should contain 330 records."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
